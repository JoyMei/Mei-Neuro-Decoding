{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K Fold Verification",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHTVjRvXRWDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64d94b0-3c32-4fbb-a286-1cdc3e352dac"
      },
      "source": [
        "!pip install keras-tuner\n",
        "!pip install mat73\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import mat73\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn.metrics\n",
        "from sklearn.utils import shuffle \n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Conv3D, MaxPooling3D, BatchNormalization\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=c568617cc6a595b5dfa5fe3cc60b93de551c7a7af1c44c6de8729b10856ca89d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=fc80b5c4cb6fa4e1094abbb70af6bb70433b1a51f4c9387548b72c0803f822fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n",
            "Collecting mat73\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/91/937025d314d66b9e868be94e190a93b5c423db16ee998a559681a81b987e/mat73-0.46-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from mat73) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mat73) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->mat73) (1.15.0)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.46\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGX33OULSUjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d04523-2b7f-42d0-ff5d-69d8b9e87ee9"
      },
      "source": [
        "mat_file = sio.loadmat('/content/drive/MyDrive/Corrected_Data/DE_data_all_corrected.mat')\n",
        "mat_file_labels = sio.loadmat('/content/drive/MyDrive/Corrected_Data/label.mat')\n",
        "count = 0\n",
        "accuracy = []\n",
        "\n",
        "data = mat_file['data']\n",
        "labels = mat_file_labels['label']\n",
        "\n",
        "labels = labels[0]\n",
        "labels_edited = np.empty(675)\n",
        "for i in range(0,45):\n",
        "  labels_edited[i*15:(i+1)*15] = labels\n",
        "\n",
        "data, labels_edited = shuffle(data, labels_edited)\n",
        "\n",
        "max = np.max(data)\n",
        "min = np.min(data)\n",
        "\n",
        "#Normalizing data\n",
        "data = data/max\n",
        "data = (data - np.mean(data))/np.std(data)\n",
        "\n",
        "#Splitting Dataset into train, validation, test \n",
        "train_labels = labels_edited[0:550]\n",
        "test_labels = labels_edited[550:600]\n",
        "train_data = data[0:550]\n",
        "test_data = data[550:600]\n",
        "final_test = data[600:675]\n",
        "final_labels = labels_edited[600:675]\n",
        "cf_labels = np.where(final_labels== -1, 2, final_labels) #Label in the form for confusion matrix\n",
        "un, co = np.unique(cf_labels, return_counts=True)\n",
        "print(f'Unique: {un}, Counts: {co}')\n",
        "\n",
        "train_labels_reshaped = train_labels.reshape(-1,1) #Formatting for input to the CNN model\n",
        "test_labels_reshaped = test_labels.reshape(-1,1)\n",
        "final_labels_reshaped = final_labels.reshape(-1,1)\n",
        "\n",
        "train_labels_reshaped = to_categorical(train_labels_reshaped, 3) #One Hot Encoding\n",
        "test_labels_reshaped = to_categorical(test_labels_reshaped, 3)\n",
        "final_labels_reshaped = to_categorical(final_labels_reshaped, 3)\n",
        "\n",
        "rnn_train = train_data.reshape(550, 62, -1) #Formatting for CNN input\n",
        "rnn_test = test_data.reshape(50, 62, -1)\n",
        "rnn_train = np.transpose(rnn_train, (0,2,1))\n",
        "rnn_test = np.transpose(rnn_test, (0,2,1))\n",
        "\n",
        "final_rnn_test = final_test.reshape(75, 62, -1)\n",
        "final_rnn_test = np.transpose(final_rnn_test, (0,2,1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique: [0. 1. 2.], Counts: [31 26 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEF1W651QSHX"
      },
      "source": [
        "#K Fold Cross Validation Test\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "cvscores = []; cvscores_rnn = []; cvscores_hybrid = []; cvscores_ensemble = []\n",
        "for train, test in kfold.split(data, labels_edited):\n",
        "  # CNN Model\n",
        "  model = Sequential([\n",
        "    Conv2D(filters= 64, kernel_size= 5, input_shape = (62,265,5), padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2,strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.3),\n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2, strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.2),\n",
        "\n",
        "    Conv2D(filters= 256, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    MaxPooling2D(pool_size= 2, strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.25),\n",
        "\n",
        "    Conv2D(filters= 512, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2, strides= 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.3),\n",
        "    \n",
        "    Flatten(),\n",
        "\n",
        "    Dense(512, activation= 'relu'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.4),\n",
        "    Dense(256, activation= 'relu'),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.2),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    #BatchNormalization(),\n",
        "    Dense(3, activation= 'softmax')\n",
        "  ])\n",
        "\n",
        "  #Compile CNN Model\n",
        "  model.compile(optimizer= keras.optimizers.Adam(learning_rate=9e-5), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])  \n",
        "  \n",
        "  #Fit the CNN model\n",
        "  reduce_lr_cnn = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-7) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history = model.fit(\n",
        "    train_data,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 64,\n",
        "    epochs=100, \n",
        "    validation_data=(test_data, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_cnn]\n",
        "  )\n",
        "\n",
        "  \n",
        "  \n",
        "  #LSTM model \n",
        "  model_rnn = Sequential([\n",
        "    LSTM(units= 64, activation= 'tanh', input_shape= [1325, 62], return_sequences = True),\n",
        "    Dropout(0.25),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.35),\n",
        "    LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(256, activation= 'relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(128, activation= 'relu'),\n",
        "    Dropout(0.35),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    Dense(3, activation= 'softmax'),\n",
        "\n",
        "  ])\n",
        "\n",
        "  #Compile LSTM Model\n",
        "  model_rnn.compile(optimizer= keras.optimizers.Adam(learning_rate=5e-5), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "  \n",
        "  #Fit LSTM Model\n",
        "  reduce_lr_rnn = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history_rnn = model_rnn.fit(\n",
        "    rnn_train,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 16,\n",
        "    epochs=60,\n",
        "    validation_data=(rnn_test, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_rnn]\n",
        "  )\n",
        "  \n",
        "  #Hybrid model \n",
        "  model_hybrid = Sequential([\n",
        "    Conv2D(filters= 64, kernel_size= 5, input_shape = (62,265,5), padding= 'same'), \n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2,strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'), \n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2, strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(0.35),\n",
        "    \n",
        "    Conv2D(filters= 256, kernel_size= 3, padding= 'same'), \n",
        "    #MaxPooling2D(pool_size= 2, strides = 2), # This wasn't here, all batch norm\n",
        "    #BatchNormalization(),\n",
        "\n",
        "    Reshape((66, 15*256), input_shape= (15, 66, 256)), #Important to reshape so data passed to LSTM Layer correctly\n",
        "    \n",
        "    LSTM(units= 64, activation= 'tanh', input_shape= [1325, 62], return_sequences = True),\n",
        "    Dropout(0.25),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.35),\n",
        "    LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "    \n",
        "    Flatten(),\n",
        "    \n",
        "    Dense(512, activation= 'relu'), \n",
        "    Dropout(0.25), \n",
        "    Dense(256, activation= 'relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    Dense(3, activation= 'softmax')\n",
        "  ])\n",
        "\n",
        "  #Compile Hybrid Model \n",
        "  model_hybrid.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  #Fit the Hybrid Model\n",
        "  reduce_lr_hybrid = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history_hybrid = model_hybrid.fit(\n",
        "    train_data,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 32,\n",
        "    epochs=60,\n",
        "    validation_data=(test_data, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_hybrid]\n",
        "  )\n",
        "  \n",
        "  #Evaluate and print on every run\n",
        "  scores = model.evaluate(x= final_test, y = final_labels_reshaped)\n",
        "  scores_rnn = model_rnn.evaluate(x= final_rnn_test, y = final_labels_reshaped)\n",
        "  scores_hybrid = model_hybrid.evaluate(x= final_test, y = final_labels_reshaped)\n",
        "\n",
        "  #Implementing Ensemble (Stacking method) using a meta model \n",
        "\n",
        "  #Generating Predictions\n",
        "  pred = np.argmax(model.predict(train_data), axis=-1)\n",
        "  pred_rnn = np.argmax(model_rnn.predict(rnn_train), axis=-1)\n",
        "  pred_hybrid = np.argmax(model_hybrid.predict(train_data), axis=-1)\n",
        "  inputs = [pred, pred_rnn, pred_hybrid]\n",
        "  inputs = np.array(inputs).T\n",
        "\n",
        "  pred_test = np.argmax(model.predict(test_data), axis=-1)\n",
        "  pred_rnn_test = np.argmax(model_rnn.predict(rnn_test), axis=-1)\n",
        "  pred_hybrid_test = np.argmax(model_hybrid.predict(test_data), axis=-1)\n",
        "  inputs_test = [pred_test, pred_rnn_test, pred_hybrid_test]\n",
        "  inputs_test = np.array(inputs_test).T\n",
        "\n",
        "  #Meta Model\n",
        "  model_stack = Sequential([\n",
        "    Dense(128, 'relu', input_shape= (3,)),\n",
        "    Dense(256, 'relu'),\n",
        "    Dense(256, 'relu'),\n",
        "    Dense(64, 'relu'),\n",
        "    Dense(3, 'softmax')\n",
        "  ])\n",
        "\n",
        "  #Compiling the Meta Model\n",
        "  model_stack.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= 'accuracy')\n",
        "\n",
        "  #Fitting the Meta Model\n",
        "  reduce_lr_stack = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history_stack = model_stack.fit(\n",
        "    inputs,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 32,\n",
        "    epochs=150,\n",
        "    validation_data=(inputs_test, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_stack]\n",
        "  )\n",
        "\n",
        "  #Evaluation, Print and store Results\n",
        "  acc = model_stack.evaluate(x= final_preds.T, y= final_labels_reshaped)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  print(\"%s: %.2f%%\" % (model_rnn.metrics_names[1], scores_rnn[1]*100))\n",
        "  print(\"%s: %.2f%%\" % (model_hybrid.metrics_names[1], scores_hybrid[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\n",
        "  cvscores_rnn.append(scores_rnn[1] * 100)\n",
        "  cvscores_hybrid.append(scores_hybrid[1] * 100)\n",
        "  cvscores_ensemble.append(acc[1])\n",
        "\n",
        "\n",
        "  print(\"%s: %.2f%%\" % (model_hybrid.metrics_names[1], scores_hybrid[1]*100))\n",
        "  cvscores_hybrid.append(scores_hybrid[1] * 100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDWVwPHibSa1"
      },
      "source": [
        "#Print results and Box and Whisker Plot for Hybrid model. Can be changed for each model.\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Hybrid K Fold Validation Test Boxplot (K=10)')\n",
        "ax.boxplot((cvscores_hybrid), showfliers=True)\n",
        "print(f'Accuracy Average: {np.mean(cvscores_hybrid)}, Standard Deviation: {np.std(cvscores_hybrid)}, Max: {np.max(cvscores_hybrid)}, Min: {np.min(cvscores_hybrid)}')\n",
        "\n",
        "#Uncomment to save results and box and whisker plots\n",
        "#np.savetxt('hybrid_accuracies.csv', cvscores_hybrid, delimiter=',')\n",
        "#plt.savefig('Hybrid_boxplot.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}